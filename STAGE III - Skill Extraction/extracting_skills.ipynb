{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import silence_tensorflow.auto\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# initializing Stop words libraries\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/') \n",
    "\n",
    "# Access the database\n",
    "db = client['job-resume-db']  \n",
    "\n",
    "# Access the collection\n",
    "collection = db['job-descriptions'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = collection.find({})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions_df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649cd598eed36cf2eea926a8</td>\n",
       "      <td>0</td>\n",
       "      <td>product manager</td>\n",
       "      <td>Title: Mobile Marketing Executive Full Descrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>649cd598eed36cf2eea926a9</td>\n",
       "      <td>1</td>\n",
       "      <td>product manager</td>\n",
       "      <td>Title: Product Manager IT Channel Harrogate **...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>649cd598eed36cf2eea926aa</td>\n",
       "      <td>2</td>\n",
       "      <td>product manager</td>\n",
       "      <td>Title: PR Account Executive/PR Account Manager...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>649cd598eed36cf2eea926ab</td>\n",
       "      <td>3</td>\n",
       "      <td>product manager</td>\n",
       "      <td>Title: Product Marketing Manager Full Descript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>649cd598eed36cf2eea926ac</td>\n",
       "      <td>4</td>\n",
       "      <td>product manager</td>\n",
       "      <td>Title: PR Account Manager Full Description: PR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  index         category  \\\n",
       "0  649cd598eed36cf2eea926a8      0  product manager   \n",
       "1  649cd598eed36cf2eea926a9      1  product manager   \n",
       "2  649cd598eed36cf2eea926aa      2  product manager   \n",
       "3  649cd598eed36cf2eea926ab      3  product manager   \n",
       "4  649cd598eed36cf2eea926ac      4  product manager   \n",
       "\n",
       "                                         description  \n",
       "0  Title: Mobile Marketing Executive Full Descrip...  \n",
       "1  Title: Product Manager IT Channel Harrogate **...  \n",
       "2  Title: PR Account Executive/PR Account Manager...  \n",
       "3  Title: Product Marketing Manager Full Descript...  \n",
       "4  Title: PR Account Manager Full Description: PR...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = job_descriptions_df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file and read the lines into a list\n",
    "with open('noisy_words.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "# Remove any newline characters from each word\n",
    "noisy_words = [word.strip() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    desc = contractions.fix(text)\n",
    "    # for word in noisy_words:\n",
    "    #     desc = re.sub(r'\\b' + word + r'\\b', '', desc)\n",
    "    desc = re.sub(\"[!@.$\\'\\'':()]\", \"\", desc)\n",
    "    tokens = nltk.word_tokenize(desc)\n",
    "    cleaned_tokens = [word for word in tokens if word not in noisy_words]\n",
    "    cleaned_description = ' '.join(cleaned_tokens)\n",
    "    return cleaned_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_tag(desc):\n",
    "    tokens = nltk.word_tokenize(desc.lower())\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(filtered_tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_POS(tagged):\n",
    "    #pattern 1\n",
    "    grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar1)\n",
    "    tree1 = chunkParser.parse(tagged)\n",
    "\n",
    "    # typical noun phrase pattern appending to be concatted later\n",
    "    g1_chunks = []\n",
    "    for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "        g1_chunks.append(subtree)\n",
    "\n",
    "    #pattern 2\n",
    "    grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar2)\n",
    "    tree2 = chunkParser.parse(tagged)\n",
    "\n",
    "    # variation of a noun phrase pattern to be pickled for later analyses\n",
    "    g2_chunks = []\n",
    "    for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "        g2_chunks.append(subtree)\n",
    "\n",
    "    #pattern 3\n",
    "    grammar3 = (''' VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar3)\n",
    "    tree3 = chunkParser.parse(tagged)\n",
    "\n",
    "    # verb-noun pattern appending to be concatted later\n",
    "    g3_chunks = []\n",
    "    for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "        g3_chunks.append(subtree)\n",
    "\n",
    "\n",
    "    # pattern 4\n",
    "    # any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "    grammar4 = ('''Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar4)\n",
    "    tree4 = chunkParser.parse(tagged)\n",
    "\n",
    "    # common pattern of listing skills appending to be concatted later\n",
    "    g4_chunks = []\n",
    "    for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "        g4_chunks.append(subtree)\n",
    "\n",
    "    return g1_chunks, g2_chunks, g3_chunks, g4_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set(chunks):\n",
    "    '''creates a dataframe that easily parsed with the chunks data '''\n",
    "    df = pd.DataFrame(chunks)\n",
    "    df.fillna('X', inplace = True)\n",
    "\n",
    "    train = []\n",
    "    for row in df.values:\n",
    "        phrase = ''\n",
    "        for tup in row:\n",
    "            # needs a space at the end for seperation\n",
    "            phrase += tup[0] + ' '\n",
    "        phrase = ''.join(phrase)\n",
    "        # could use padding tages but encoder method will provide during\n",
    "        # tokenizing/embeddings; X can replace paddding for now\n",
    "        train.append( phrase.replace('X', '').strip())\n",
    "\n",
    "    df['phrase'] = train\n",
    "\n",
    "    #returns 50% of each dataframe to be used if you want to improve execution time\n",
    "    # return df.phrase.sample(frac = 0.5)\n",
    "    # Update: only do 50% if running on excel\n",
    "    return df.phrase\n",
    "\n",
    "def strip_commas(df):\n",
    "    '''create new series of individual n-grams'''\n",
    "    grams = []\n",
    "    for sen in df:\n",
    "        sent = sen.split(',')\n",
    "        for word in sent:\n",
    "            grams.append(word)\n",
    "    return pd.Series(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrases(desc):\n",
    "    desc = clean_text(desc)\n",
    "    tagged = tokenize_and_tag(desc)\n",
    "    g1_chunks, g2_chunks, g3_chunks, g4_chunks = extract_POS(tagged)\n",
    "    c = training_set(g4_chunks)\n",
    "    separated_chunks4 = strip_commas(c)\n",
    "    phrases = pd.concat([training_set(g1_chunks),\n",
    "                          training_set(g2_chunks),\n",
    "                          training_set(g3_chunks),\n",
    "                          separated_chunks4],\n",
    "                            ignore_index = True )\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458cf15c5bed46dea0f988e8019c6479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Phrases:   0%|          | 0/3530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extracted_POS_from_all_job_descriptions = []\n",
    "\n",
    "# Assuming job_descriptions is a list of all your job descriptions\n",
    "for desc in tqdm(job_descriptions, desc='Generating Phrases: '):\n",
    "    phrases = generate_phrases(desc)\n",
    "    extracted_POS_from_all_job_descriptions.append(phrases)\n",
    "\n",
    "# Concatenate all series\n",
    "all_phrases = pd.concat(extracted_POS_from_all_job_descriptions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              title\n",
       "1         mobile marketing executive\n",
       "2                   full description\n",
       "3                       appointments\n",
       "4                         passionate\n",
       "                     ...            \n",
       "516607                        please\n",
       "516608                        posted\n",
       "516609                waterlooville \n",
       "516610             hampshire client \n",
       "516611                              \n",
       "Length: 516612, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_words = list(set(noisy_words))\n",
    "\n",
    "def preprocess_phrases(text):\n",
    "    # remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    # remove leading and trailing slashes\n",
    "    text = text.strip('/')\n",
    "    # remove noisy words\n",
    "    text = ' '.join(word for word in text.split() if word not in noisy_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows that are not in the noisy_words list\n",
    "filtered_phrases = all_phrases[~all_phrases.isin(noisy_words)]\n",
    "\n",
    "# Keep rows that don't contain special characters except '-', '/', and '.'\n",
    "filtered_phrases = filtered_phrases[filtered_phrases.apply(lambda x: bool(re.match('^[a-zA-Z-/\\. ]*$', x)))]\n",
    "\n",
    "filtered_phrases = filtered_phrases.apply(lambda row: preprocess_phrases(row))\n",
    "\n",
    "# Remove empty strings\n",
    "filtered_phrases = filtered_phrases[filtered_phrases != '']\n",
    "\n",
    "# Remove duplicates\n",
    "filtered_phrases = filtered_phrases.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only phrases with at most 3 words\n",
    "filtered_phrases = filtered_phrases[filtered_phrases.apply(lambda x: len(x.split(' ')) <= 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_phrases.to_csv('phrases.csv',index=False) # Collect the phrases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Optimizing LSTM for classifying phrases as skill or no-skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras_tuner.tuners import RandomSearch, BayesianOptimization\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom column names\n",
    "custom_columns = ['Text', 'Skills/No_Skills']\n",
    "\n",
    "# Read the Excel file and specify the custom column names\n",
    "skills_data = pd.read_excel('skills_no-skills_backup.xlsx', names=custom_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Skills/No_Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a/b</td>\n",
       "      <td>skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a/c reconciliations</td>\n",
       "      <td>skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaai qualifications</td>\n",
       "      <td>skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aac</td>\n",
       "      <td>skill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aareon qlx</td>\n",
       "      <td>No-skill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Text Skills/No_Skills\n",
       "0                  a/b            skill\n",
       "1  a/c reconciliations            skill\n",
       "2  aaai qualifications            skill\n",
       "3                  aac            skill\n",
       "4           aareon qlx         No-skill"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "skills_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['skill', 'No-skill', 'No-Skill', 'Skill', 'no-skill'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_data['Skills/No_Skills'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the 'Skills/No_Skills' column\n",
    "skills_data['Skills/No_Skills'] = skills_data['Skills/No_Skills'].replace('No-skill', 'no-skill')\\\n",
    ".replace('Skill', 'skill')\\\n",
    ".replace('No-Skill','no-skill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['skill', 'no-skill'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_data['Skills/No_Skills'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Replace '/' with space\n",
    "    text = text.replace('/', ' ')\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())  # Remove extra spaces and replace with single space\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the \"Text\" column, but keep the first 5 rows unchanged\n",
    "first_five_rows = skills_data['Text'].iloc[:5]  # Extract the first five rows\n",
    "rest_of_rows = skills_data['Text'].iloc[5:]  # Extract the rest of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    a/b\n",
       "1    a/c reconciliations\n",
       "2    aaai qualifications\n",
       "3                    aac\n",
       "4             aareon qlx\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5                                       aat\n",
       "6                                  aat acca\n",
       "7                      aat close completion\n",
       "8                       aat junior accounts\n",
       "9                             aat qualified\n",
       "                        ...                \n",
       "67739                                rutter\n",
       "67740                             rwi media\n",
       "67741                          ryan henshaw\n",
       "67742           ryanhaywardpeoplesourcecouk\n",
       "67743    ryanhaywardpeoplesourcecouk source\n",
       "Name: Text, Length: 67733, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the rest of the rows\n",
    "rest_of_rows = rest_of_rows.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    a/b\n",
       "1    a/c reconciliations\n",
       "2    aaai qualifications\n",
       "3                    aac\n",
       "4             aareon qlx\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the first five rows with the processed rest of the rows\n",
    "skills_data['Text'] = pd.concat([first_five_rows, rest_of_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the skills\n",
    "skills_data[skills_data['Skills/No_Skills'] == 'skill'].to_csv('skills.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = skills_data['Text'].values\n",
    "labels = skills_data['Skills/No_Skills'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "encoded_docs = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max([len(s.split()) for s in texts])\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, encoded_labels, test_size=0.2, stratify=encoded_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tokenizer_information = {\n",
    "    'tokenizer':tokenizer,\n",
    "    'max_length':max_length\n",
    "}\n",
    "\n",
    "file_name = 'tokenizer_saved_information.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(tokenizer_information, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Optimization(hp):\n",
    "    embedding_dim = hp.Int('embedding_dims', min_value=50, max_value=300)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(SpatialDropout1D(hp.Float('Spatial_Dropout', min_value=0.1, max_value=0.3, step=0.05)))\n",
    "\n",
    "    # Add LSTM layers dynamically based on hyperparameter search\n",
    "    for i in range(hp.Int('n_layers', 1, 5)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units', min_value=8, max_value=100, step=32), return_sequences=True))\n",
    "        model.add(Dropout(hp.Float(f'Dropout_rate_{i}', min_value=0, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(hp.Int('layer_2_neurons', min_value=8, max_value=100, step=32)))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate_last', min_value=0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Optimize the Adam optimizer using Keras Tuner\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])  # Set learning rate choices\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=hp_learning_rate), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True,mode='max')\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(\n",
    "                                                         monitor = \"val_loss\",\n",
    "                                                         factor = 0.1,\n",
    "                                                         patience = 3,\n",
    "                                                         verbose = 0,\n",
    "                                                         mode = \"min\"\n",
    "                                                         )\n",
    "\n",
    "# define a callback to clear the training outputs at the end of every training step\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 53s]\n",
      "val_accuracy: 0.872453510761261\n",
      "\n",
      "Best val_accuracy So Far: 0.8851491212844849\n",
      "Total elapsed time: 00h 08m 41s\n"
     ]
    }
   ],
   "source": [
    "# Perform RandomSearch with Keras Tuner\n",
    "tuner = BayesianOptimization(LSTM_Optimization,\n",
    "                              objective='val_accuracy',\n",
    "                              max_trials=5,\n",
    "                              directory='Optimizing LSTM',\n",
    "                              project_name='skill_no-skill_classify')\n",
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_test, y_test),\n",
    "                          batch_size=512,\n",
    "                          callbacks = [ClearTrainingOutput(),\n",
    "                          early_stopping,\n",
    "                          reduce_lr_on_plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lstm_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 16, 190)           2376710   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 16, 190)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 16, 40)            36960     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 40)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 16, 8)             1568      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 8)             0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 16, 8)             544       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16, 8)             0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 16, 8)             544       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 8)             0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 16, 8)             544       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16, 8)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 72)                23328     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 72)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 73        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,440,271\n",
      "Trainable params: 2,440,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 5s 7ms/step - loss: 0.3218 - accuracy: 0.8851\n",
      "Testing Accuracy:  0.8851\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = best_lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f2795e2286426c9c313362d0f970cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Phrases:   0%|          | 0/3530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_job_descriptions = []\n",
    "\n",
    "for desc in tqdm(job_descriptions, desc='Generating Phrases: '):\n",
    "    desc = clean_text(desc)\n",
    "    cleaned_job_descriptions.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills_using_lstm(trained_lstm_model, raw_job_description, tokenizer, max_length, noisy_words_path = 'noisy_words.txt'):\n",
    "    # Tokenize and tag the new job description\n",
    "    tagged_description = tokenize_and_tag(raw_job_description)\n",
    "\n",
    "    # Extract parts of speech\n",
    "    g1_chunks, g2_chunks, g3_chunks, g4_chunks = extract_POS(tagged_description)\n",
    "\n",
    "    c = training_set(g4_chunks)\n",
    "    separated_chunks4 = strip_commas(c)\n",
    "    phrases = pd.concat([training_set(g1_chunks),\n",
    "                        training_set(g2_chunks),\n",
    "                        training_set(g3_chunks),\n",
    "                        separated_chunks4],\n",
    "                            ignore_index = True)\n",
    "    \n",
    "    # Open the text file and read the lines into a list\n",
    "    with open(noisy_words_path, 'r') as f:\n",
    "        words = f.readlines()\n",
    "\n",
    "    # Remove any newline characters from each word\n",
    "    noisy_words = [word.strip() for word in words]\n",
    "    \n",
    "    noisy_words = list(set(noisy_words))\n",
    "\n",
    "    def preprocess_phrases(text):\n",
    "        # remove leading and trailing spaces\n",
    "        text = text.strip()\n",
    "        # remove leading and trailing slashes\n",
    "        text = text.strip('/')\n",
    "        # remove noisy words\n",
    "        text = ' '.join(word for word in text.split() if word not in noisy_words)\n",
    "        return text\n",
    "    \n",
    "    # Keep rows that are not in the noisy_words list\n",
    "    filtered_phrases = phrases[~phrases.isin(noisy_words)]\n",
    "\n",
    "    # Keep rows that don't contain special characters except '-', '/', and '.'\n",
    "    filtered_phrases = filtered_phrases[filtered_phrases.apply(lambda x: bool(re.match('^[a-zA-Z-/\\. ]*$', x)))]\n",
    "\n",
    "    filtered_phrases = filtered_phrases.apply(lambda row: preprocess_phrases(row))\n",
    "\n",
    "    # Remove empty strings\n",
    "    filtered_phrases = filtered_phrases[filtered_phrases != '']\n",
    "\n",
    "    # Remove duplicates\n",
    "    filtered_phrases = filtered_phrases.drop_duplicates()\n",
    "\n",
    "    # Keep only phrases with at most 3 words\n",
    "    filtered_phrases = filtered_phrases[filtered_phrases.apply(lambda x: len(x.split(' ')) <= 3)]\n",
    "\n",
    "    # create array of text\n",
    "    text = np.array(filtered_phrases)\n",
    "\n",
    "    # convert the array of text into sequence\n",
    "    encoded_docs = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    # pad the text sequence to make them equal length\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "    # perform the prediction using trained lstm model\n",
    "    predictions = (trained_lstm_model.predict(padded_docs) > 0.5).astype('int32')\n",
    "\n",
    "    # perform the prediction using lstm\n",
    "    out = pd.DataFrame({'Phrase':filtered_phrases, 'Class':predictions.ravel()})\n",
    "\n",
    "    # extract the skill from the predicted classification of phrases\n",
    "    skills = out.loc[out['Class'] == 1]\n",
    "\n",
    "    return skills['Phrase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_extraction_steps_information = {\n",
    "    'clean_text':clean_text,\n",
    "    'tokenize_and_tag':tokenize_and_tag,\n",
    "    'extract_POS':extract_POS,\n",
    "    'training_set':training_set,\n",
    "    'tokenizer':tokenizer,\n",
    "    'pad_sequences':pad_sequences,\n",
    "    'max_length':max_length,\n",
    "    'extract_skills_using_lstm':extract_skills_using_lstm\n",
    "}\n",
    "\n",
    "file_name = 'skill_extraction_steps_information.pkl'\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(skill_extraction_steps_information, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['net/sql',\n",
       " 'cms',\n",
       " 'content management cms',\n",
       " 'social networking',\n",
       " 'new product features',\n",
       " 'aspnet c',\n",
       " 'html',\n",
       " 'xhtml',\n",
       " 'asp net developer',\n",
       " 'sql server',\n",
       " 'javascript',\n",
       " 'css',\n",
       " 'xml',\n",
       " 'ajax',\n",
       " 'asp net programmer',\n",
       " 'programmer',\n",
       " 'software engineer job',\n",
       " 'net framework',\n",
       " 'software developer',\n",
       " 'intranet cms',\n",
       " 'networking',\n",
       " 'c']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills_using_lstm(trained_lstm_model=best_lstm_model,\n",
    "                          raw_job_description=cleaned_job_descriptions[2050],\n",
    "                          tokenizer=tokenizer,\n",
    "                          max_length=max_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the trained LSTM model\n",
    "best_lstm_model.save('trained_lstm_model_for_extracting_skills.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-2-job-description-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

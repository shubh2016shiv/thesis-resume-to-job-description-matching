{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements_resume_parsing.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "25bb97fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyresparser import ResumeParser\n",
    "from pymongo import MongoClient\n",
    "from PyPDF4 import PdfFileReader\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "from tika import parser\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcf34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Paths\n",
    "chartered_accountant_resume_path = 'resume/chartered accountant/'\n",
    "machine_learning_resume_path = 'resume/machine learning/'\n",
    "software_developer_resume_path = 'resume/software developer/'\n",
    "product_manager_resume_path = 'resume/product manager/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "831fdf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chartered Accountant Resume: 5\n"
     ]
    }
   ],
   "source": [
    "# Resume for chartered accountant\n",
    "chartered_accountant_resume = [resume_file for resume_file in os.listdir(chartered_accountant_resume_path)]\n",
    "print(f\"Number of Chartered Accountant Resume: {len(chartered_accountant_resume)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323d584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Machine Learning Resume: 6\n"
     ]
    }
   ],
   "source": [
    "# Resume for machine learning\n",
    "machine_learning_resume = [resume_file for resume_file in os.listdir(machine_learning_resume_path)]\n",
    "print(f\"Number of Machine Learning Resume: {len(machine_learning_resume)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ade2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Product Manager Resume: 1\n"
     ]
    }
   ],
   "source": [
    "# Resume for product manager\n",
    "product_manager_resume  = [resume_file for resume_file in os.listdir(product_manager_resume_path)]\n",
    "print(f\"Number of Product Manager Resume: {len(product_manager_resume)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5413f04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Software Developer Resume: 9\n"
     ]
    }
   ],
   "source": [
    "# Resume for software developer\n",
    "software_developer_resume = [resume_file for resume_file in os.listdir(software_developer_resume_path)]\n",
    "print(f\"Number of Software Developer Resume: {len(software_developer_resume)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "429a348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF file using Tika and perform basic cleaning\n",
    "def remove_words_matching_patterns(text):\n",
    "    patterns = ['outlook', 'gmail', 'linkedin', r'\\b\\w+com\\b']\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    cleaned_text = re.sub(combined_pattern, '', text, flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def extract_and_clean_text_from_pdf(file_path):\n",
    "    raw = parser.from_file(file_path)\n",
    "    text = raw['content']\n",
    "    # Remove special characters and extra whitespaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Remove LinkedIn or GitHub URLs\n",
    "    text = re.sub(r'(https?:\\/\\/(?:www\\.)?linkedin\\.com\\/[^\\s]+)|(https?:\\/\\/(?:www\\.)?github\\.com\\/[^\\s]+)', '', text)\n",
    "    # Remove GitHub or Gmail addresses\n",
    "    text = re.sub(r'(github|gmail)[\\w.]*', '', text, flags=re.IGNORECASE)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "     # Remove email addresses with domain extensions\n",
    "    text = re.sub(r'\\S+@(?:gmail|outlook)\\.(?:com)', '', text)\n",
    "    # Remove specific phrases like \"gmail.com\"\n",
    "    text = re.sub(r'\\bgmail\\.com\\b', '', text)\n",
    "    # Remove GitHub usernames\n",
    "    text = re.sub(r'\\bgithub\\.com\\/\\w+\\b', '', text)\n",
    "    # Remove special characters and extra whitespaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "     # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    return str.lower(remove_words_matching_patterns(' '.join(lemmatized_tokens)))\n",
    "\n",
    "# Function to parse and convert resume to JSON format\n",
    "def parse_and_jsonize_resume(resume_path, resume_file):\n",
    "    parsed_data = ResumeParser(resume_path + resume_file).get_extracted_data()\n",
    "    return json.dumps(parsed_data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a6c44528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB configuration\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['job-resume-db']\n",
    "collection = db['resume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e19d3593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting, Parsing Resume and Storing in MongoDB: 100%|██████████| 4/4 [00:28<00:00,  7.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing the resume PDFs\n",
    "resume_folder = 'resume'\n",
    "\n",
    "# Delete all existing documents in the collection\n",
    "collection.delete_many({})\n",
    "\n",
    "# Iterate over each folder in the resume directory\n",
    "for category in tqdm(os.listdir(resume_folder), desc=\"Extracting, Parsing Resume and Storing in MongoDB: \"):\n",
    "    category_folder = os.path.join(resume_folder, category)\n",
    "    if os.path.isdir(category_folder):\n",
    "        # Iterate over each resume PDF in the category folder\n",
    "        for resume_file in os.listdir(category_folder):\n",
    "            resume_path = os.path.join(category_folder, resume_file)\n",
    "            \n",
    "            # Extract and clean the text from the resume PDF\n",
    "            cleaned_text = extract_and_clean_text_from_pdf(resume_path)\n",
    "            \n",
    "            # Parse and convert resume to JSON format\n",
    "            parsed_json = parse_and_jsonize_resume(category_folder + '/', resume_file)\n",
    "            \n",
    "            # Create a dictionary with the resume information\n",
    "            resume_data = {\n",
    "                'index': resume_file,\n",
    "                'category': category,\n",
    "                'text': cleaned_text,\n",
    "                'parsed_resume': json.loads(parsed_json)\n",
    "            }\n",
    "            \n",
    "            # Insert the resume data into the MongoDB collection\n",
    "            collection.insert_one(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a999c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
